{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c0884aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Creating pickle file ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "try:\n",
    "    import urllib.request\n",
    "except ImportError:\n",
    "    raise ImportError('You should use Python 3.x')\n",
    "import os.path\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "url_base = 'http://yann.lecun.com/exdb/mnist/'\n",
    "key_file = {\n",
    "    'train_img':'train-images-idx3-ubyte.gz',\n",
    "    'train_label':'train-labels-idx1-ubyte.gz',\n",
    "    'test_img':'t10k-images-idx3-ubyte.gz',\n",
    "    'test_label':'t10k-labels-idx1-ubyte.gz'\n",
    "}\n",
    "\n",
    "dataset_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "save_file = dataset_dir + \"/mnist.pkl\"\n",
    "\n",
    "train_num = 60000\n",
    "test_num = 10000\n",
    "img_dim = (1, 28, 28)\n",
    "img_size = 784\n",
    "\n",
    "\n",
    "def _download(file_name):\n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        return\n",
    "\n",
    "    print(\"Downloading \" + file_name + \" ... \")\n",
    "    urllib.request.urlretrieve(url_base + file_name, file_path)\n",
    "    print(\"Done\")\n",
    "    \n",
    "def download_mnist():\n",
    "    for v in key_file.values():\n",
    "       _download(v)\n",
    "        \n",
    "def _load_label(file_name):\n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "    \n",
    "    print(\"Converting \" + file_name + \" to NumPy Array ...\")\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "            labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def _load_img(file_name):\n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "    \n",
    "    print(\"Converting \" + file_name + \" to NumPy Array ...\")    \n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "    data = data.reshape(-1, img_size)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    return data\n",
    "    \n",
    "def _convert_numpy():\n",
    "    dataset = {}\n",
    "    dataset['train_img'] =  _load_img(key_file['train_img'])\n",
    "    dataset['train_label'] = _load_label(key_file['train_label'])    \n",
    "    dataset['test_img'] = _load_img(key_file['test_img'])\n",
    "    dataset['test_label'] = _load_label(key_file['test_label'])\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def init_mnist():\n",
    "    download_mnist()\n",
    "    dataset = _convert_numpy()\n",
    "    print(\"Creating pickle file ...\")\n",
    "    with open(save_file, 'wb') as f:\n",
    "        pickle.dump(dataset, f, -1)\n",
    "    print(\"Done!\")\n",
    "\n",
    "def _change_ont_hot_label(X):\n",
    "    T = np.zeros((X.size, 10))\n",
    "    for idx, row in enumerate(T):\n",
    "        row[X[idx]] = 1\n",
    "        \n",
    "    return T\n",
    "    \n",
    "\n",
    "def load_mnist(normalize=True, flatten=True, one_hot_label=False):\n",
    "    \"\"\"MNIST 데이터셋 읽기\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    normalize : 이미지의 픽셀 값을 0.0~1.0 사이의 값으로 정규화할지 정한다.\n",
    "    one_hot_label : \n",
    "        one_hot_label이 True면、레이블을 원-핫(one-hot) 배열로 돌려준다.\n",
    "        one-hot 배열은 예를 들어 [0,0,1,0,0,0,0,0,0,0]처럼 한 원소만 1인 배열이다.\n",
    "    flatten : 입력 이미지를 1차원 배열로 만들지를 정한다. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (훈련 이미지, 훈련 레이블), (시험 이미지, 시험 레이블)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_file):\n",
    "        init_mnist()\n",
    "        \n",
    "    with open(save_file, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    \n",
    "    if normalize:\n",
    "        for key in ('train_img', 'test_img'):\n",
    "            dataset[key] = dataset[key].astype(np.float32)\n",
    "            dataset[key] /= 255.0\n",
    "            \n",
    "    if one_hot_label:\n",
    "        dataset['train_label'] = _change_ont_hot_label(dataset['train_label'])\n",
    "        dataset['test_label'] = _change_ont_hot_label(dataset['test_label'])    \n",
    "    \n",
    "    if not flatten:\n",
    "         for key in ('train_img', 'test_img'):\n",
    "            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n",
    "\n",
    "    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label']) \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    init_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e14ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.util import shuffle_dataset\n",
    "from common.trainer import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286f1011",
   "metadata": {},
   "source": [
    "1. 검증 데이터 셋 이용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52765b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist()\n",
    "\n",
    "x_train, t_train = shuffle_dataset(x_train, t_train)\n",
    "\n",
    "validation_rate = 0.20\n",
    "validation_num = int(x_train.shape[0] * validation_rate)\n",
    "\n",
    "x_val = x_train[:validation_num]\n",
    "t_val = t_train[:validation_num]\n",
    "x_train = x_train[validation_num:]\n",
    "t_train = t_train[validation_num:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4588d0ac",
   "metadata": {},
   "source": [
    "2. 하이퍼 파라미터 최적화 => 그리드서치 비슷함<br>\n",
    "1) 하이퍼파라미터 값 범위 설정<br>\n",
    "2) 설정된 범위에서 하이퍼파라미터 값 무작위 추출<br>\n",
    "3) 학습/검증 데이터로 평가<br>\n",
    "4) 1)~3)을 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9370eb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc:0.94925 | lr:0.0013985358729311386, weight decay:1.5597117108125317e-07\n",
      "val acc:0.60275 | lr:3.954973854999814e-05, weight decay:6.635518223737095e-07\n",
      "val acc:0.32916666666666666 | lr:1.6019887454910137e-05, weight decay:7.789566882779855e-07\n",
      "val acc:0.9630833333333333 | lr:0.003417416330678635, weight decay:2.5231763358067155e-06\n",
      "val acc:0.9500833333333333 | lr:0.0014259467926820357, weight decay:4.673237431967048e-07\n",
      "val acc:0.658 | lr:3.8772077364095064e-05, weight decay:6.112364828685405e-05\n",
      "val acc:0.4745 | lr:2.4869514839301357e-05, weight decay:2.4818634077826823e-07\n",
      "val acc:0.23116666666666666 | lr:7.774415596250021e-06, weight decay:1.0159002069280112e-06\n",
      "val acc:0.16108333333333333 | lr:9.897129626415545e-06, weight decay:4.108727318851387e-08\n",
      "val acc:0.14108333333333334 | lr:3.963914382534753e-06, weight decay:8.089768070585652e-05\n",
      "val acc:0.11733333333333333 | lr:2.5369513501697093e-06, weight decay:6.228858237528483e-06\n",
      "val acc:0.18166666666666667 | lr:6.0521009858334784e-06, weight decay:2.1828092432679935e-07\n",
      "val acc:0.11808333333333333 | lr:7.784435973324684e-06, weight decay:1.863570198351272e-08\n",
      "val acc:0.9438333333333333 | lr:0.0008630833671548387, weight decay:6.351635885516411e-06\n",
      "val acc:0.5033333333333333 | lr:2.7523880026598205e-05, weight decay:1.7670316500568533e-05\n",
      "val acc:0.9584166666666667 | lr:0.0022220310359054073, weight decay:1.664880391719534e-07\n",
      "val acc:0.19483333333333333 | lr:1.1131815530529993e-05, weight decay:9.478252602566583e-06\n",
      "val acc:0.9514166666666667 | lr:0.0014933134771420071, weight decay:3.036423946702205e-08\n",
      "val acc:0.0965 | lr:1.8367111073433536e-06, weight decay:4.4760109129795485e-06\n",
      "val acc:0.8164166666666667 | lr:8.436342204693029e-05, weight decay:1.5361427893029823e-06\n",
      "val acc:0.9684166666666667 | lr:0.006357150434800245, weight decay:4.1738257888091414e-07\n",
      "val acc:0.8550833333333333 | lr:0.00010821348851555841, weight decay:3.410607407203464e-07\n",
      "val acc:0.16341666666666665 | lr:1.3984141815237648e-06, weight decay:4.749078551662358e-05\n",
      "val acc:0.9666666666666667 | lr:0.004473548023830645, weight decay:1.6315692097471351e-06\n",
      "val acc:0.33016666666666666 | lr:2.1447812899523885e-05, weight decay:1.1003005652759966e-07\n",
      "val acc:0.10833333333333334 | lr:3.514789255961935e-06, weight decay:3.1917769187822416e-07\n",
      "val acc:0.78 | lr:7.877260556416405e-05, weight decay:9.0599160329648e-06\n",
      "val acc:0.9654166666666667 | lr:0.004644664103822775, weight decay:5.922867412582905e-08\n",
      "val acc:0.8905833333333333 | lr:0.00023350114659195944, weight decay:6.9273067523470934e-06\n"
     ]
    }
   ],
   "source": [
    "def __train(lr, weight_decay, epocs = 50):\n",
    "    network = MultiLayerNet(input_size = 784,\n",
    "                           hidden_size_list = [100, 100, 100, 100, 100, 100],\n",
    "                           output_size = 10, weight_decay_lambda = weight_decay)\n",
    "    \n",
    "    trainer = Trainer(network, x_train, t_train, x_val, t_val,\n",
    "                     epochs = epocs, mini_batch_size = 100,\n",
    "                     optimizer = 'sgd',\n",
    "                     optimizer_param = {'lr' : lr},\n",
    "                     verbose = False)\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    return trainer.test_acc_list, trainer.train_acc_list\n",
    "\n",
    "\n",
    "optimization_trial = 100\n",
    "results_val =  {}\n",
    "results_train = {}\n",
    "\n",
    "for _ in range(optimization_trial):\n",
    "    # 탐색한 하이퍼파라미터의 범위 지정===============\n",
    "    weight_decay = 10 ** np.random.uniform(-8, -4)\n",
    "    lr = 10 ** np.random.uniform(-6, -2)\n",
    "    # ================================================\n",
    "\n",
    "    val_acc_list, train_acc_list = __train(lr, weight_decay)\n",
    "    print(\"val acc:\" + str(val_acc_list[-1]) + \" | lr:\" + str(lr) + \", weight decay:\" + str(weight_decay))\n",
    "    key = \"lr:\" + str(lr) + \", weight decay:\" + str(weight_decay)\n",
    "    results_val[key] = val_acc_list\n",
    "    results_train[key] = train_acc_list\n",
    "    \n",
    "\n",
    "print(\"=========== Hyper-Parameter Optimization Result ===========\")\n",
    "graph_draw_num = 20\n",
    "col_num = 5\n",
    "row_num = int(np.ceil(graph_draw_num / col_num))\n",
    "i = 0\n",
    "\n",
    "for key, val_acc_list in sorted(results_val.items(), key=lambda x: x[1][-1], reverse=True):\n",
    "    print(\"Best-\" + str(i+1) + \"(val acc:\" + str(val_acc_list[-1]) + \") | \" + key)\n",
    "\n",
    "    plt.subplot(row_num, col_num, i+1)\n",
    "    plt.title(\"Best-\" + str(i+1))\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    if i % 5:\n",
    "        plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    x = np.arange(len(val_acc_list))\n",
    "    plt.plot(x, val_acc_list)\n",
    "    plt.plot(x, results_train[key], \"--\")\n",
    "    i += 1\n",
    "\n",
    "    if i >= graph_draw_num:\n",
    "        break\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7038a333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006a6e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fcbca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b636211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f54372e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29079d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ed22ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce93e802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b5d58b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f829f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bdb21a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3fb917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
