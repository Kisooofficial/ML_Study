{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fffc9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] 지정된 파일을 찾을 수 없습니다: 'Chapter07'\n",
      "C:\\Users\\revol\\Chapter07\n"
     ]
    }
   ],
   "source": [
    "cd Chapter07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c113df58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz ... \n",
      "Done\n",
      "Downloading train-labels-idx1-ubyte.gz ... \n",
      "Done\n",
      "Downloading t10k-images-idx3-ubyte.gz ... \n",
      "Done\n",
      "Downloading t10k-labels-idx1-ubyte.gz ... \n",
      "Done\n",
      "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Creating pickle file ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "try:\n",
    "    import urllib.request\n",
    "except ImportError:\n",
    "    raise ImportError('You should use Python 3.x')\n",
    "import os.path\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "url_base = 'http://yann.lecun.com/exdb/mnist/'\n",
    "key_file = {\n",
    "    'train_img':'train-images-idx3-ubyte.gz',\n",
    "    'train_label':'train-labels-idx1-ubyte.gz',\n",
    "    'test_img':'t10k-images-idx3-ubyte.gz',\n",
    "    'test_label':'t10k-labels-idx1-ubyte.gz'\n",
    "}\n",
    "\n",
    "dataset_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "save_file = dataset_dir + \"/mnist.pkl\"\n",
    "\n",
    "train_num = 60000\n",
    "test_num = 10000\n",
    "img_dim = (1, 28, 28)\n",
    "img_size = 784\n",
    "\n",
    "\n",
    "def _download(file_name):\n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        return\n",
    "\n",
    "    print(\"Downloading \" + file_name + \" ... \")\n",
    "    urllib.request.urlretrieve(url_base + file_name, file_path)\n",
    "    print(\"Done\")\n",
    "    \n",
    "def download_mnist():\n",
    "    for v in key_file.values():\n",
    "       _download(v)\n",
    "        \n",
    "def _load_label(file_name):\n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "    \n",
    "    print(\"Converting \" + file_name + \" to NumPy Array ...\")\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "            labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def _load_img(file_name):\n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "    \n",
    "    print(\"Converting \" + file_name + \" to NumPy Array ...\")    \n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "    data = data.reshape(-1, img_size)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    return data\n",
    "    \n",
    "def _convert_numpy():\n",
    "    dataset = {}\n",
    "    dataset['train_img'] =  _load_img(key_file['train_img'])\n",
    "    dataset['train_label'] = _load_label(key_file['train_label'])    \n",
    "    dataset['test_img'] = _load_img(key_file['test_img'])\n",
    "    dataset['test_label'] = _load_label(key_file['test_label'])\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def init_mnist():\n",
    "    download_mnist()\n",
    "    dataset = _convert_numpy()\n",
    "    print(\"Creating pickle file ...\")\n",
    "    with open(save_file, 'wb') as f:\n",
    "        pickle.dump(dataset, f, -1)\n",
    "    print(\"Done!\")\n",
    "\n",
    "def _change_ont_hot_label(X):\n",
    "    T = np.zeros((X.size, 10))\n",
    "    for idx, row in enumerate(T):\n",
    "        row[X[idx]] = 1\n",
    "        \n",
    "    return T\n",
    "    \n",
    "\n",
    "def load_mnist(normalize=True, flatten=True, one_hot_label=False):\n",
    "    \"\"\"MNIST 데이터셋 읽기\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    normalize : 이미지의 픽셀 값을 0.0~1.0 사이의 값으로 정규화할지 정한다.\n",
    "    one_hot_label : \n",
    "        one_hot_label이 True면、레이블을 원-핫(one-hot) 배열로 돌려준다.\n",
    "        one-hot 배열은 예를 들어 [0,0,1,0,0,0,0,0,0,0]처럼 한 원소만 1인 배열이다.\n",
    "    flatten : 입력 이미지를 1차원 배열로 만들지를 정한다. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (훈련 이미지, 훈련 레이블), (시험 이미지, 시험 레이블)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_file):\n",
    "        init_mnist()\n",
    "        \n",
    "    with open(save_file, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    \n",
    "    if normalize:\n",
    "        for key in ('train_img', 'test_img'):\n",
    "            dataset[key] = dataset[key].astype(np.float32)\n",
    "            dataset[key] /= 255.0\n",
    "            \n",
    "    if one_hot_label:\n",
    "        dataset['train_label'] = _change_ont_hot_label(dataset['train_label'])\n",
    "        dataset['test_label'] = _change_ont_hot_label(dataset['test_label'])    \n",
    "    \n",
    "    if not flatten:\n",
    "         for key in ('train_img', 'test_img'):\n",
    "            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n",
    "\n",
    "    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label']) \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    init_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c92023cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.pardir)\n",
    "from common.util import im2col\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from common.trainer import Trainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e71aff7",
   "metadata": {},
   "source": [
    "x1 : 채널이 3개고, 채널 1개당 7 * 7데이터임, 필터는 5 * 5, 그래서 계산과정 9번이라 생각하면 됌.<br>\n",
    "필터적용영역 1개를 가로로 쭉 펼치는 거니까 9개가 나옴.(9 * 25) 여기에 채널 3개를 옆에다가 붙이면 9 * 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "256712ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.pardir)\n",
    "from common.util import im2col\n",
    "\n",
    "x1 = np.random.rand(1, 3, 7, 7)\n",
    "col1 = im2col(x1, 5, 5, stride = 1, pad = 0)\n",
    "print(col1.shape)\n",
    "\n",
    "x2 = np.random.rand(10, 3, 7, 7)\n",
    "col2 = im2col(x2, 5, 5, stride =1 , pad = 0)\n",
    "print(col2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e90a9f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        # 중간 데이터（backward 시 사용）\n",
    "        self.x = None   \n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        # 가중치와 편향 매개변수의 기울기\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "042c855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride = 1, pad = 0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "        \n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h * self.pool_w)\n",
    "        \n",
    "        arg_max = np.argmax(col, axis = 1)\n",
    "        out = np.max(col, axis = 1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0295970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleConvNet:\n",
    "    \"\"\"\n",
    "    다음과 같은 CNN을 구성한다.\n",
    "    → Conv → ReLU → Pooling → Affine → ReLU → Affine → Softmax →\n",
    "    전체 구현은 simple_convnet.py 참고\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param={'filter_num': 30, 'filter_size': 5,\n",
    "                             'pad': 0, 'stride': 1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        # 초기화 인수로 주어진 하이퍼파라미터를 딕셔너리에서 꺼내고 출력 크기를 계산한다.\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / \\\n",
    "            filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) *\n",
    "                               (conv_output_size/2))\n",
    "\n",
    "        # 1층의 합성곱 계층과 2, 3층의 완전연결 계층의 가중치와 편향 생성\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # CNN을 구성하는 계층을 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'],\n",
    "                                           self.params['b1'],\n",
    "                                           conv_param['stride'],\n",
    "                                           conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"추론을 수행\"\"\"\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"손실함수 값 계산\"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"오차역전파법으로 기울기를 구함\"\"\"\n",
    "        # 순전파\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # 역전파\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "882f21fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2992482722759324\n",
      "=== epoch:1, train acc:0.31, test acc:0.295 ===\n",
      "train loss:2.2968702543211394\n",
      "train loss:2.291402420040946\n",
      "train loss:2.2838029526280508\n",
      "train loss:2.2781397583643983\n",
      "train loss:2.2630675281934867\n",
      "train loss:2.25078586847934\n",
      "train loss:2.2403130974254366\n",
      "train loss:2.205330139988918\n",
      "train loss:2.189071275841958\n",
      "train loss:2.145585362575498\n",
      "train loss:2.106172872848768\n",
      "train loss:2.09844306320091\n",
      "train loss:2.002590398062306\n",
      "train loss:1.9514590372802243\n",
      "train loss:1.8997964942476682\n",
      "train loss:1.8225829036519174\n",
      "train loss:1.7005209812132218\n",
      "train loss:1.65093084842943\n",
      "train loss:1.5502548828858411\n",
      "train loss:1.464838636353573\n",
      "train loss:1.4657267991640168\n",
      "train loss:1.207798629138668\n",
      "train loss:1.2323189595699189\n",
      "train loss:1.2188873246798702\n",
      "train loss:1.0270284974842046\n",
      "train loss:1.0501332663277112\n",
      "train loss:0.9564333590846411\n",
      "train loss:0.7781037529959945\n",
      "train loss:0.887559319658332\n",
      "train loss:0.8740857448757966\n",
      "train loss:0.7556315432499656\n",
      "train loss:0.6110170583118182\n",
      "train loss:0.7715267451901887\n",
      "train loss:0.6273661034526037\n",
      "train loss:0.8241275746649335\n",
      "train loss:0.6059642262209838\n",
      "train loss:0.5225932897865644\n",
      "train loss:0.6196836025590499\n",
      "train loss:0.6981526614831312\n",
      "train loss:0.5378646072939933\n",
      "train loss:0.6056867155495476\n",
      "train loss:0.4390904155348768\n",
      "train loss:0.5019878125308054\n",
      "train loss:0.4066173645253852\n",
      "train loss:0.5299316617618256\n",
      "train loss:0.38383693885922265\n",
      "train loss:0.49985746712350376\n",
      "train loss:0.5961504071442871\n",
      "train loss:0.5974129611734037\n",
      "train loss:0.5450665187952901\n",
      "=== epoch:2, train acc:0.814, test acc:0.808 ===\n",
      "train loss:0.42467626731349206\n",
      "train loss:0.6529804620898271\n",
      "train loss:0.6615224119775385\n",
      "train loss:0.5546386127441412\n",
      "train loss:0.30719346775618106\n",
      "train loss:0.4733331428290319\n",
      "train loss:0.4846749552935848\n",
      "train loss:0.38613858932534\n",
      "train loss:0.37441843148245296\n",
      "train loss:0.5651922978203705\n",
      "train loss:0.37732678203300646\n",
      "train loss:0.42117469020283776\n",
      "train loss:0.3591600316751433\n",
      "train loss:0.5122774131772355\n",
      "train loss:0.37078676277401235\n",
      "train loss:0.4055012779431634\n",
      "train loss:0.42171618464803085\n",
      "train loss:0.460020433452355\n",
      "train loss:0.37056219837945775\n",
      "train loss:0.47204443660282813\n",
      "train loss:0.37948396501013315\n",
      "train loss:0.41485918674822647\n",
      "train loss:0.3937202917607521\n",
      "train loss:0.4129011676216766\n",
      "train loss:0.4542043208869391\n",
      "train loss:0.5145160054890946\n",
      "train loss:0.3043394124064055\n",
      "train loss:0.3224251602247058\n",
      "train loss:0.35396661685337577\n",
      "train loss:0.32112252562264343\n",
      "train loss:0.49716453654635706\n",
      "train loss:0.40655512429706897\n",
      "train loss:0.4437101766112774\n",
      "train loss:0.3952927327369403\n",
      "train loss:0.3049661078081288\n",
      "train loss:0.23480562753319684\n",
      "train loss:0.3733555363762017\n",
      "train loss:0.34219117086466627\n",
      "train loss:0.4646586053883746\n",
      "train loss:0.3575446847131347\n",
      "train loss:0.5562663818619569\n",
      "train loss:0.2726907088520718\n",
      "train loss:0.5039385005525925\n",
      "train loss:0.19896113918808156\n",
      "train loss:0.4166818315301779\n",
      "train loss:0.2837472484419668\n",
      "train loss:0.25684232997109857\n",
      "train loss:0.17769173009670264\n",
      "train loss:0.3824059836551947\n",
      "train loss:0.2760725139678637\n",
      "=== epoch:3, train acc:0.885, test acc:0.871 ===\n",
      "train loss:0.3323679819114925\n",
      "train loss:0.3441766190168641\n",
      "train loss:0.24236594401554487\n",
      "train loss:0.23258440292696253\n",
      "train loss:0.48584937440351084\n",
      "train loss:0.47744686315483004\n",
      "train loss:0.2937908398282888\n",
      "train loss:0.40517278920341454\n",
      "train loss:0.3961950234171186\n",
      "train loss:0.26098384172063593\n",
      "train loss:0.3394631690495193\n",
      "train loss:0.353964716901907\n",
      "train loss:0.3888138985975111\n",
      "train loss:0.28843797122745607\n",
      "train loss:0.1815041968013496\n",
      "train loss:0.23370090218961745\n",
      "train loss:0.2323883280279308\n",
      "train loss:0.2331015427405284\n",
      "train loss:0.20988465885184532\n",
      "train loss:0.2685308107327527\n",
      "train loss:0.32218524857215775\n",
      "train loss:0.2769701737806218\n",
      "train loss:0.47269138352399975\n",
      "train loss:0.22067057121424571\n",
      "train loss:0.19432134484273178\n",
      "train loss:0.1508233976570638\n",
      "train loss:0.2657091252507252\n",
      "train loss:0.22668913775262126\n",
      "train loss:0.22412325077947026\n",
      "train loss:0.24621103078580434\n",
      "train loss:0.25500668603266397\n",
      "train loss:0.26273676805511914\n",
      "train loss:0.35749621262900644\n",
      "train loss:0.432369065741788\n",
      "train loss:0.3357580649830233\n",
      "train loss:0.19233343349367682\n",
      "train loss:0.25623910183538595\n",
      "train loss:0.18633731899424466\n",
      "train loss:0.23086923310739782\n",
      "train loss:0.18218300532177806\n",
      "train loss:0.4056779078248934\n",
      "train loss:0.5312311963981\n",
      "train loss:0.1730089065779617\n",
      "train loss:0.2609204832876223\n",
      "train loss:0.2749190024255942\n",
      "train loss:0.2324480227471002\n",
      "train loss:0.3088157316018807\n",
      "train loss:0.26028364734188364\n",
      "train loss:0.2969913894738657\n",
      "train loss:0.1525046851871833\n",
      "=== epoch:4, train acc:0.886, test acc:0.88 ===\n",
      "train loss:0.34103128808273453\n",
      "train loss:0.32602560635289585\n",
      "train loss:0.22967467936609676\n",
      "train loss:0.20635205120098185\n",
      "train loss:0.3305610999324496\n",
      "train loss:0.16245907460626338\n",
      "train loss:0.22831380121095596\n",
      "train loss:0.37550027532326946\n",
      "train loss:0.22322765124702698\n",
      "train loss:0.4644755402877118\n",
      "train loss:0.25154330734950475\n",
      "train loss:0.27559543754244503\n",
      "train loss:0.2038211697927122\n",
      "train loss:0.3331777461848073\n",
      "train loss:0.17397156708248804\n",
      "train loss:0.18832728318851924\n",
      "train loss:0.32224264409357056\n",
      "train loss:0.37545096975195397\n",
      "train loss:0.25898960765365514\n",
      "train loss:0.26328704848777057\n",
      "train loss:0.25478721145502364\n",
      "train loss:0.2896796323948528\n",
      "train loss:0.41128386365012753\n",
      "train loss:0.2699909391046458\n",
      "train loss:0.1725083188023515\n",
      "train loss:0.32687012616416944\n",
      "train loss:0.2676937416885988\n",
      "train loss:0.18558346401555872\n",
      "train loss:0.21702987546789743\n",
      "train loss:0.11843310387489694\n",
      "train loss:0.31698334046429827\n",
      "train loss:0.25080559947107095\n",
      "train loss:0.21830731385521795\n",
      "train loss:0.19546634855254902\n",
      "train loss:0.3599543722873902\n",
      "train loss:0.3359667480531547\n",
      "train loss:0.21792034837001736\n",
      "train loss:0.2911354350732704\n",
      "train loss:0.20068225191344058\n",
      "train loss:0.2640844151656044\n",
      "train loss:0.25252242227101696\n",
      "train loss:0.13698191466407875\n",
      "train loss:0.2005803362052879\n",
      "train loss:0.2778411996521778\n",
      "train loss:0.20370544479177563\n",
      "train loss:0.2156225589062825\n",
      "train loss:0.16757397753536765\n",
      "train loss:0.28844337188729063\n",
      "train loss:0.12822646673733684\n",
      "train loss:0.18835294239455674\n",
      "=== epoch:5, train acc:0.923, test acc:0.919 ===\n",
      "train loss:0.2696291095245633\n",
      "train loss:0.27686978966876274\n",
      "train loss:0.23311380031628898\n",
      "train loss:0.13366923217285373\n",
      "train loss:0.23102064754258658\n",
      "train loss:0.1332254219308988\n",
      "train loss:0.18316343744314806\n",
      "train loss:0.27399027956160293\n",
      "train loss:0.11840985012928446\n",
      "train loss:0.32045664366649246\n",
      "train loss:0.2453919955740855\n",
      "train loss:0.15548566102978734\n",
      "train loss:0.25185142565005547\n",
      "train loss:0.14966780086375994\n",
      "train loss:0.1941620370930136\n",
      "train loss:0.2992502589108973\n",
      "train loss:0.18881517544241813\n",
      "train loss:0.1554481444571135\n",
      "train loss:0.20413348890276958\n",
      "train loss:0.17717805088791136\n",
      "train loss:0.15602345171602192\n",
      "train loss:0.16953352221252024\n",
      "train loss:0.204635599344283\n",
      "train loss:0.19369273373819085\n",
      "train loss:0.16349580374804318\n",
      "train loss:0.2132418859659721\n",
      "train loss:0.20748806014624166\n",
      "train loss:0.10856107582788342\n",
      "train loss:0.1437257220584289\n",
      "train loss:0.28447134354442866\n",
      "train loss:0.3025242477862593\n",
      "train loss:0.1830657498621507\n",
      "train loss:0.2602188338413302\n",
      "train loss:0.19558661338618247\n",
      "train loss:0.3290955074398637\n",
      "train loss:0.2875651949202251\n",
      "train loss:0.2510817743819543\n",
      "train loss:0.11049806183623553\n",
      "train loss:0.16065915520190238\n",
      "train loss:0.19822956295272706\n",
      "train loss:0.27583851172766494\n",
      "train loss:0.28654778490562194\n",
      "train loss:0.14098530215880453\n",
      "train loss:0.16114763001923385\n",
      "train loss:0.22953631435830882\n",
      "train loss:0.20439092095095873\n",
      "train loss:0.18847054879451558\n",
      "train loss:0.15438777148120633\n",
      "train loss:0.1434147993621698\n",
      "train loss:0.11819309303565624\n",
      "=== epoch:6, train acc:0.934, test acc:0.915 ===\n",
      "train loss:0.2758772885201625\n",
      "train loss:0.25244243600150007\n",
      "train loss:0.44468699148606056\n",
      "train loss:0.14383615814687795\n",
      "train loss:0.19344050628721854\n",
      "train loss:0.14505933242966149\n",
      "train loss:0.13529167438217807\n",
      "train loss:0.22454184063378296\n",
      "train loss:0.08690482667707937\n",
      "train loss:0.19405401763154184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.08154920364987632\n",
      "train loss:0.09690423537463957\n",
      "train loss:0.12778860370207265\n",
      "train loss:0.11230334567426128\n",
      "train loss:0.12471722731254152\n",
      "train loss:0.10914239004510959\n",
      "train loss:0.2499287346804386\n",
      "train loss:0.19800151702549126\n",
      "train loss:0.24758273979618464\n",
      "train loss:0.08693488583478688\n",
      "train loss:0.16503392178095935\n",
      "train loss:0.13787379094526683\n",
      "train loss:0.07493040021503478\n",
      "train loss:0.09988494575757216\n",
      "train loss:0.11180287314866916\n",
      "train loss:0.18770857168198343\n",
      "train loss:0.09602731295683195\n",
      "train loss:0.10173088940832384\n",
      "train loss:0.10394455573557476\n",
      "train loss:0.14886146712392498\n",
      "train loss:0.133750648815539\n",
      "train loss:0.21896865753962885\n",
      "train loss:0.10382707310123714\n",
      "train loss:0.30054565097271096\n",
      "train loss:0.1305694407852433\n",
      "train loss:0.21345026159538044\n",
      "train loss:0.13505561353076068\n",
      "train loss:0.1489339962021632\n",
      "train loss:0.07567406286798707\n",
      "train loss:0.3520993718745497\n",
      "train loss:0.1285243945065785\n",
      "train loss:0.07999348402850978\n",
      "train loss:0.11251458230869421\n",
      "train loss:0.138770744378434\n",
      "train loss:0.20035808731334598\n",
      "train loss:0.12479472624919183\n",
      "train loss:0.2341180549498598\n",
      "train loss:0.1303024847797969\n",
      "train loss:0.09467185195562969\n",
      "train loss:0.27450246791357313\n",
      "=== epoch:7, train acc:0.943, test acc:0.929 ===\n",
      "train loss:0.1752363655773796\n",
      "train loss:0.2152886576900273\n",
      "train loss:0.0634164150081598\n",
      "train loss:0.13084768585267703\n",
      "train loss:0.15101760849924542\n",
      "train loss:0.15131477629796344\n",
      "train loss:0.1473895054480969\n",
      "train loss:0.13442839388080988\n",
      "train loss:0.18827933476108805\n",
      "train loss:0.12710039205346343\n",
      "train loss:0.17566965581141195\n",
      "train loss:0.2674435056080889\n",
      "train loss:0.15774178342636758\n",
      "train loss:0.19741214351700725\n",
      "train loss:0.15550548034456665\n",
      "train loss:0.12877716934921918\n",
      "train loss:0.1068622762688462\n",
      "train loss:0.27454153353008937\n",
      "train loss:0.11956351247088821\n",
      "train loss:0.05902527737173832\n",
      "train loss:0.2156625415580289\n",
      "train loss:0.2441063093791326\n",
      "train loss:0.17728942936543246\n",
      "train loss:0.1725602426916727\n",
      "train loss:0.1987457880353075\n",
      "train loss:0.13496869144706314\n",
      "train loss:0.14529054916062112\n",
      "train loss:0.09622553147514568\n",
      "train loss:0.13722157038619012\n",
      "train loss:0.13547303576455974\n",
      "train loss:0.12103638143970086\n",
      "train loss:0.08236487654984423\n",
      "train loss:0.09627821007115905\n",
      "train loss:0.11086766975392046\n",
      "train loss:0.0742567069436419\n",
      "train loss:0.09303312818655436\n",
      "train loss:0.12509616587498237\n",
      "train loss:0.24417090847909326\n",
      "train loss:0.2566763379758029\n",
      "train loss:0.1362793482139632\n",
      "train loss:0.08763974072290616\n",
      "train loss:0.10979247120512997\n",
      "train loss:0.1248529804096041\n",
      "train loss:0.12803533414300755\n",
      "train loss:0.1447261036768114\n",
      "train loss:0.23005464379712712\n",
      "train loss:0.1730314753627915\n",
      "train loss:0.07368358087514462\n",
      "train loss:0.07544264571413352\n",
      "train loss:0.1191680090843677\n",
      "=== epoch:8, train acc:0.957, test acc:0.936 ===\n",
      "train loss:0.1229375836328223\n",
      "train loss:0.16922731913750794\n",
      "train loss:0.11768804502828623\n",
      "train loss:0.09024129156743099\n",
      "train loss:0.07677774484613271\n",
      "train loss:0.14525460649625022\n",
      "train loss:0.13986202361549363\n",
      "train loss:0.16421878762269834\n",
      "train loss:0.08915122945471315\n",
      "train loss:0.06124125249656091\n",
      "train loss:0.09169804317142063\n",
      "train loss:0.08371922239005065\n",
      "train loss:0.13581244787196184\n",
      "train loss:0.1473287304705388\n",
      "train loss:0.16990936167708967\n",
      "train loss:0.08984207664783693\n",
      "train loss:0.10184455321295506\n",
      "train loss:0.07789385782423443\n",
      "train loss:0.18751817446096022\n",
      "train loss:0.11021559114176319\n",
      "train loss:0.18440993884337306\n",
      "train loss:0.054425538257523234\n",
      "train loss:0.04420896395722587\n",
      "train loss:0.08126232702733448\n",
      "train loss:0.03836058910362647\n",
      "train loss:0.15143172243674902\n",
      "train loss:0.11783049588404929\n",
      "train loss:0.06304616864398231\n",
      "train loss:0.12426583533211837\n",
      "train loss:0.04242116719478692\n",
      "train loss:0.19255024065705625\n",
      "train loss:0.14130901345630278\n",
      "train loss:0.14952099241986264\n",
      "train loss:0.07708026476138116\n",
      "train loss:0.1457566330800584\n",
      "train loss:0.05267302980933537\n",
      "train loss:0.08917857039661645\n",
      "train loss:0.15330098726738306\n",
      "train loss:0.0970966626397227\n",
      "train loss:0.06927205570155552\n",
      "train loss:0.06825060519048325\n",
      "train loss:0.08277763893811602\n",
      "train loss:0.08917827995792148\n",
      "train loss:0.08485698715418291\n",
      "train loss:0.1628790096966185\n",
      "train loss:0.16565767262797976\n",
      "train loss:0.10686334990935249\n",
      "train loss:0.08332192095729239\n",
      "train loss:0.10713271234280851\n",
      "train loss:0.06346200759118732\n",
      "=== epoch:9, train acc:0.957, test acc:0.941 ===\n",
      "train loss:0.13750409775505118\n",
      "train loss:0.051355879030924714\n",
      "train loss:0.09850191266264313\n",
      "train loss:0.09812014520889713\n",
      "train loss:0.16483664691550942\n",
      "train loss:0.07719480409822572\n",
      "train loss:0.10898819360421921\n",
      "train loss:0.037017527046153825\n",
      "train loss:0.1466581923021662\n",
      "train loss:0.11539398890543681\n",
      "train loss:0.13316224393304835\n",
      "train loss:0.050099685131379114\n",
      "train loss:0.07438469354232854\n",
      "train loss:0.08313323017617266\n",
      "train loss:0.07768237549223668\n",
      "train loss:0.06609026653262724\n",
      "train loss:0.06766806761989616\n",
      "train loss:0.026931539112131708\n",
      "train loss:0.08926557885025849\n",
      "train loss:0.11742779438706323\n",
      "train loss:0.050355497968516516\n",
      "train loss:0.15146040054169002\n",
      "train loss:0.13733612444658228\n",
      "train loss:0.10399924737992855\n",
      "train loss:0.06603961247154755\n",
      "train loss:0.0902804006249391\n",
      "train loss:0.07538229248551441\n",
      "train loss:0.07018518850358332\n",
      "train loss:0.19940119590889752\n",
      "train loss:0.08223542996776198\n",
      "train loss:0.0740126768899047\n",
      "train loss:0.10245442493914286\n",
      "train loss:0.05881100195724013\n",
      "train loss:0.1763998583495108\n",
      "train loss:0.04499921430778257\n",
      "train loss:0.047835595628415675\n",
      "train loss:0.038424204657233486\n",
      "train loss:0.07512098499983216\n",
      "train loss:0.11181497496882847\n",
      "train loss:0.03483311721742757\n",
      "train loss:0.14410755901978137\n",
      "train loss:0.1021180178644427\n",
      "train loss:0.12146561558830499\n",
      "train loss:0.10875951802185949\n",
      "train loss:0.042579352063402735\n",
      "train loss:0.05651513040008399\n",
      "train loss:0.05222696763393557\n",
      "train loss:0.057874547525301356\n",
      "train loss:0.05298848232316126\n",
      "train loss:0.0397218660511859\n",
      "=== epoch:10, train acc:0.963, test acc:0.942 ===\n",
      "train loss:0.18121061332976235\n",
      "train loss:0.06892134601193069\n",
      "train loss:0.0782566049431529\n",
      "train loss:0.13372305660254372\n",
      "train loss:0.05951733919139917\n",
      "train loss:0.05151780453221686\n",
      "train loss:0.039819570488238344\n",
      "train loss:0.04097834361070874\n",
      "train loss:0.07985399747100357\n",
      "train loss:0.08513588754739944\n",
      "train loss:0.06858766428935155\n",
      "train loss:0.09033104270094063\n",
      "train loss:0.04607850463036173\n",
      "train loss:0.07041720106205328\n",
      "train loss:0.09080668938325244\n",
      "train loss:0.08222210889987742\n",
      "train loss:0.028630922654633722\n",
      "train loss:0.09612147608490197\n",
      "train loss:0.05218495530651886\n",
      "train loss:0.04410534600205564\n",
      "train loss:0.1440683294030718\n",
      "train loss:0.04275106226122824\n",
      "train loss:0.053111559401803225\n",
      "train loss:0.0932773589370248\n",
      "train loss:0.04951915439311836\n",
      "train loss:0.03603479103102325\n",
      "train loss:0.08986311973156891\n",
      "train loss:0.06023776372090259\n",
      "train loss:0.038312795042889265\n",
      "train loss:0.06301213348617254\n",
      "train loss:0.05805604823203079\n",
      "train loss:0.0586166279245273\n",
      "train loss:0.08709746197398145\n",
      "train loss:0.13037196175403054\n",
      "train loss:0.07448880667384561\n",
      "train loss:0.07024738184795891\n",
      "train loss:0.04276046077078128\n",
      "train loss:0.0744205283393197\n",
      "train loss:0.10453416257232957\n",
      "train loss:0.13661194384526565\n",
      "train loss:0.08807664901509725\n",
      "train loss:0.0460875073601969\n",
      "train loss:0.05129653818495121\n",
      "train loss:0.08659327792954198\n",
      "train loss:0.12605179609380512\n",
      "train loss:0.1101961377959474\n",
      "train loss:0.06027194042820173\n",
      "train loss:0.058518794649391304\n",
      "train loss:0.03842276421244771\n",
      "train loss:0.15207116806623142\n",
      "=== epoch:11, train acc:0.967, test acc:0.948 ===\n",
      "train loss:0.0757783025488699\n",
      "train loss:0.07341844403025431\n",
      "train loss:0.025947439060321317\n",
      "train loss:0.026128812498857488\n",
      "train loss:0.05272494715747169\n",
      "train loss:0.02825145176144566\n",
      "train loss:0.15357570063099688\n",
      "train loss:0.026365330221102948\n",
      "train loss:0.13136757994232634\n",
      "train loss:0.030782739944440453\n",
      "train loss:0.07032688581894622\n",
      "train loss:0.07061921504119381\n",
      "train loss:0.1147176893210794\n",
      "train loss:0.03098083172426394\n",
      "train loss:0.12426155708434744\n",
      "train loss:0.05682576187738257\n",
      "train loss:0.043563489593041294\n",
      "train loss:0.11364351782733385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.0824368550963062\n",
      "train loss:0.03867704344154018\n",
      "train loss:0.06229681355851528\n",
      "train loss:0.040887065378158255\n",
      "train loss:0.06435039345671985\n",
      "train loss:0.04368264589009697\n",
      "train loss:0.04159761427555484\n",
      "train loss:0.03896159654405396\n",
      "train loss:0.09868257307470324\n",
      "train loss:0.044923730252715205\n",
      "train loss:0.07069209255753335\n",
      "train loss:0.028159057991141774\n",
      "train loss:0.02594388134247546\n",
      "train loss:0.07358267906692278\n",
      "train loss:0.04165701093004936\n",
      "train loss:0.03868534162535969\n",
      "train loss:0.043764760599973834\n",
      "train loss:0.09894196065036098\n",
      "train loss:0.09954254369790468\n",
      "train loss:0.04979902109243409\n",
      "train loss:0.030312175464897995\n",
      "train loss:0.14417032479155079\n",
      "train loss:0.1360570142027201\n",
      "train loss:0.09593818065512667\n",
      "train loss:0.11281343094610718\n",
      "train loss:0.031249452256527963\n",
      "train loss:0.06396428870833457\n",
      "train loss:0.08177048844040932\n",
      "train loss:0.030311371919063714\n",
      "train loss:0.07113596110411348\n",
      "train loss:0.028956148225123636\n",
      "train loss:0.039418097112110875\n",
      "=== epoch:12, train acc:0.971, test acc:0.945 ===\n",
      "train loss:0.04607003822341636\n",
      "train loss:0.06425900787040767\n",
      "train loss:0.0693890936567645\n",
      "train loss:0.07008949166174493\n",
      "train loss:0.08784849163310617\n",
      "train loss:0.05966926080314195\n",
      "train loss:0.055466718938268364\n",
      "train loss:0.04182033596536647\n",
      "train loss:0.08940365330816649\n",
      "train loss:0.05826759196588215\n",
      "train loss:0.0552962235550836\n",
      "train loss:0.08471063704127285\n",
      "train loss:0.08193313787458494\n",
      "train loss:0.1569112052365891\n",
      "train loss:0.09551631371446324\n",
      "train loss:0.0807822497027333\n",
      "train loss:0.07229854787763193\n",
      "train loss:0.08771975116906834\n",
      "train loss:0.09213873038886154\n",
      "train loss:0.07813231285722537\n",
      "train loss:0.09385115810432541\n",
      "train loss:0.1441549132814285\n",
      "train loss:0.10907777358758784\n",
      "train loss:0.058626094931487856\n",
      "train loss:0.1856620985830109\n",
      "train loss:0.09813616584753233\n",
      "train loss:0.05427923157659196\n",
      "train loss:0.03732652290160494\n",
      "train loss:0.06470272376939065\n",
      "train loss:0.07095509363389901\n",
      "train loss:0.0459196524083872\n",
      "train loss:0.05670068809744214\n",
      "train loss:0.07602387905871341\n",
      "train loss:0.04160123095517985\n",
      "train loss:0.04976368299875902\n",
      "train loss:0.038946173730229905\n",
      "train loss:0.09830746616710676\n",
      "train loss:0.08988555766241324\n",
      "train loss:0.04146053468880856\n",
      "train loss:0.05568131332143853\n",
      "train loss:0.09998372421996854\n",
      "train loss:0.09078254316793632\n",
      "train loss:0.05025306943127342\n",
      "train loss:0.0855591842918707\n",
      "train loss:0.025731824056821742\n",
      "train loss:0.01762540870722018\n",
      "train loss:0.1049810602902248\n",
      "train loss:0.056218326567206066\n",
      "train loss:0.15097389922768145\n",
      "train loss:0.02921229803085374\n",
      "=== epoch:13, train acc:0.976, test acc:0.959 ===\n",
      "train loss:0.038452882298942\n",
      "train loss:0.06457581013419966\n",
      "train loss:0.10980223224645534\n",
      "train loss:0.042969877460229934\n",
      "train loss:0.07036547729963825\n",
      "train loss:0.06517984095380383\n",
      "train loss:0.055831520624174076\n",
      "train loss:0.09866520535644627\n",
      "train loss:0.04322733616308749\n",
      "train loss:0.05848956148709633\n",
      "train loss:0.03438518135022093\n",
      "train loss:0.1456148644398817\n",
      "train loss:0.042828236661842024\n",
      "train loss:0.021964360649486446\n",
      "train loss:0.08827324069596922\n",
      "train loss:0.04090299011602022\n",
      "train loss:0.04463815717890937\n",
      "train loss:0.02800998948396958\n",
      "train loss:0.11085342679432746\n",
      "train loss:0.037434763368039844\n",
      "train loss:0.03394203440287039\n",
      "train loss:0.05301878065958032\n",
      "train loss:0.06935654720521485\n",
      "train loss:0.03767726037400143\n",
      "train loss:0.04709558038451818\n",
      "train loss:0.056117026649378496\n",
      "train loss:0.017037193919320213\n",
      "train loss:0.06528670511464074\n",
      "train loss:0.03603053720565538\n",
      "train loss:0.0387997305198469\n",
      "train loss:0.030291720436220863\n",
      "train loss:0.04207366786469438\n",
      "train loss:0.08515099126966758\n",
      "train loss:0.07458141013660546\n",
      "train loss:0.05431057539838627\n",
      "train loss:0.02201584132806365\n",
      "train loss:0.024106267677446266\n",
      "train loss:0.0527756281108394\n",
      "train loss:0.0442392666799487\n",
      "train loss:0.07269975942263845\n",
      "train loss:0.033222236380314844\n",
      "train loss:0.1417858146770074\n",
      "train loss:0.01442962623387309\n",
      "train loss:0.061763984199925764\n",
      "train loss:0.025886795795884536\n",
      "train loss:0.02646443359652184\n",
      "train loss:0.06692083248860946\n",
      "train loss:0.08745165727321748\n",
      "train loss:0.06111536865321825\n",
      "train loss:0.0504152862736085\n",
      "=== epoch:14, train acc:0.978, test acc:0.955 ===\n",
      "train loss:0.029898760636180383\n",
      "train loss:0.03436471884321148\n",
      "train loss:0.04722810359168271\n",
      "train loss:0.07128334828232813\n",
      "train loss:0.04060240043864407\n",
      "train loss:0.02679366229481879\n",
      "train loss:0.07844633365364094\n",
      "train loss:0.06198913442701863\n",
      "train loss:0.05846607958979816\n",
      "train loss:0.06322256766063124\n",
      "train loss:0.033582399849615745\n",
      "train loss:0.03521371323088325\n",
      "train loss:0.038552787049117104\n",
      "train loss:0.0393658250727812\n",
      "train loss:0.04683642584666461\n",
      "train loss:0.06290096536069918\n",
      "train loss:0.042031539410151524\n",
      "train loss:0.02305693926620009\n",
      "train loss:0.021190606204882043\n",
      "train loss:0.08502756018429924\n",
      "train loss:0.05175856449475592\n",
      "train loss:0.07409953148607011\n",
      "train loss:0.07037960440769218\n",
      "train loss:0.03443537310583937\n",
      "train loss:0.03985619284908835\n",
      "train loss:0.035283395501189856\n",
      "train loss:0.06577180818638788\n",
      "train loss:0.03986412944815585\n",
      "train loss:0.0313605305540049\n",
      "train loss:0.0493294877565022\n",
      "train loss:0.09652438803340953\n",
      "train loss:0.027878569737164512\n",
      "train loss:0.04397675506695295\n",
      "train loss:0.0789989563220634\n",
      "train loss:0.030653983491215425\n",
      "train loss:0.02156197880109079\n",
      "train loss:0.022787529944188818\n",
      "train loss:0.014633533825353165\n",
      "train loss:0.06295529086627262\n",
      "train loss:0.015325872587170847\n",
      "train loss:0.038786068781445214\n",
      "train loss:0.015638054820687943\n",
      "train loss:0.018105446547159055\n",
      "train loss:0.04755277921270437\n",
      "train loss:0.045196614824638645\n",
      "train loss:0.028215476806945024\n",
      "train loss:0.01981562380990355\n",
      "train loss:0.010999126969869077\n",
      "train loss:0.059142135679188274\n",
      "train loss:0.02778617241628134\n",
      "=== epoch:15, train acc:0.982, test acc:0.961 ===\n",
      "train loss:0.061549019035714976\n",
      "train loss:0.026700129145534754\n",
      "train loss:0.036573576294708116\n",
      "train loss:0.038220323474904026\n",
      "train loss:0.013031211719643634\n",
      "train loss:0.06587631595641055\n",
      "train loss:0.026483367842394138\n",
      "train loss:0.07522401135331905\n",
      "train loss:0.035321212263847095\n",
      "train loss:0.0437879107730544\n",
      "train loss:0.060399153050256664\n",
      "train loss:0.0190416082993532\n",
      "train loss:0.03759987615087825\n",
      "train loss:0.046205378263949076\n",
      "train loss:0.06712333070999685\n",
      "train loss:0.03267082209472969\n",
      "train loss:0.01703816874317838\n",
      "train loss:0.02325210799144593\n",
      "train loss:0.12239694012023118\n",
      "train loss:0.044573499558988025\n",
      "train loss:0.053247741622906665\n",
      "train loss:0.027068134962316314\n",
      "train loss:0.05061962172569851\n",
      "train loss:0.02147529972589719\n",
      "train loss:0.036100719267345205\n",
      "train loss:0.03284567132617667\n",
      "train loss:0.03760945397954154\n",
      "train loss:0.05548205804602127\n",
      "train loss:0.01978236069589371\n",
      "train loss:0.02682186578965579\n",
      "train loss:0.0348032190197317\n",
      "train loss:0.025196166794359955\n",
      "train loss:0.026991680984308458\n",
      "train loss:0.012462363456347145\n",
      "train loss:0.04807001013264165\n",
      "train loss:0.012113424537136559\n",
      "train loss:0.09899438730166002\n",
      "train loss:0.06919791265627563\n",
      "train loss:0.022468026987986445\n",
      "train loss:0.02162423016302022\n",
      "train loss:0.01597990879858662\n",
      "train loss:0.03371157554872659\n",
      "train loss:0.03721606833674939\n",
      "train loss:0.015954670995874633\n",
      "train loss:0.03820625286087011\n",
      "train loss:0.026636744698421998\n",
      "train loss:0.11353929790924196\n",
      "train loss:0.09103480547136802\n",
      "train loss:0.03957656018301157\n",
      "train loss:0.05440799656221579\n",
      "=== epoch:16, train acc:0.982, test acc:0.958 ===\n",
      "train loss:0.04224443045369844\n",
      "train loss:0.048184387713842716\n",
      "train loss:0.02766235415807445\n",
      "train loss:0.04024742751834697\n",
      "train loss:0.021521802606329868\n",
      "train loss:0.03382864010843696\n",
      "train loss:0.06253456210725467\n",
      "train loss:0.03034685229617362\n",
      "train loss:0.03522000191855728\n",
      "train loss:0.09583148815530342\n",
      "train loss:0.09646107832616414\n",
      "train loss:0.05992349811564275\n",
      "train loss:0.028095121701574927\n",
      "train loss:0.027141145191459687\n",
      "train loss:0.02078029980390596\n",
      "train loss:0.030274078009391258\n",
      "train loss:0.03701105883443222\n",
      "train loss:0.05819410805699322\n",
      "train loss:0.07536174584379272\n",
      "train loss:0.07234470629812115\n",
      "train loss:0.032559935230018607\n",
      "train loss:0.0096268448321209\n",
      "train loss:0.03435012793738938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.05401895587373792\n",
      "train loss:0.02002030774863941\n",
      "train loss:0.02751692112622898\n",
      "train loss:0.016642624318960634\n",
      "train loss:0.06361394709754457\n",
      "train loss:0.027780478083959184\n",
      "train loss:0.014138351480087259\n",
      "train loss:0.02896601307003199\n",
      "train loss:0.03348720527082642\n",
      "train loss:0.02824369598745461\n",
      "train loss:0.030031603187786235\n",
      "train loss:0.055996915361082644\n",
      "train loss:0.021312862014384083\n",
      "train loss:0.016839054958072645\n",
      "train loss:0.022891518321738472\n",
      "train loss:0.0156921319297635\n",
      "train loss:0.023552102225698462\n",
      "train loss:0.022018693488795527\n",
      "train loss:0.02054252770918513\n",
      "train loss:0.020700329130526284\n",
      "train loss:0.11174170650254556\n",
      "train loss:0.055104489337195285\n",
      "train loss:0.049434759350723674\n",
      "train loss:0.014260344076934557\n",
      "train loss:0.019347290334178906\n",
      "train loss:0.03840408656337234\n",
      "train loss:0.03061736982330254\n",
      "=== epoch:17, train acc:0.988, test acc:0.964 ===\n",
      "train loss:0.04112388869686729\n",
      "train loss:0.11919232577051371\n",
      "train loss:0.022003391890480548\n",
      "train loss:0.0192000017639959\n",
      "train loss:0.04148428807725919\n",
      "train loss:0.04080329152422053\n",
      "train loss:0.030599179343836956\n",
      "train loss:0.03819679557316723\n",
      "train loss:0.03002092506189723\n",
      "train loss:0.020177799442986886\n",
      "train loss:0.010765518898202946\n",
      "train loss:0.03943913465374225\n",
      "train loss:0.021507781977336254\n",
      "train loss:0.07708422043810849\n",
      "train loss:0.017523446081536092\n",
      "train loss:0.02218418176773008\n",
      "train loss:0.06389627141222347\n",
      "train loss:0.00784260741738356\n",
      "train loss:0.039318847360453525\n",
      "train loss:0.015218258538408832\n",
      "train loss:0.02601462917604751\n",
      "train loss:0.047575067529370144\n",
      "train loss:0.02165388310485126\n",
      "train loss:0.058489817077920225\n",
      "train loss:0.03605391117402908\n",
      "train loss:0.026756272101177515\n",
      "train loss:0.023817817689423478\n",
      "train loss:0.03301791667311473\n",
      "train loss:0.023673031179785396\n",
      "train loss:0.039656778216899155\n",
      "train loss:0.04133389653446543\n",
      "train loss:0.02399006201701213\n",
      "train loss:0.024899410749721657\n",
      "train loss:0.01872850645512295\n",
      "train loss:0.039665539019151805\n",
      "train loss:0.016317028012474886\n",
      "train loss:0.006793850750945539\n",
      "train loss:0.05059139770450192\n",
      "train loss:0.029220429158684436\n",
      "train loss:0.024968993214712\n",
      "train loss:0.08150019593879401\n",
      "train loss:0.03153242913195952\n",
      "train loss:0.05334360847360008\n",
      "train loss:0.02584746624964321\n",
      "train loss:0.012278381768733923\n",
      "train loss:0.038359466163006985\n",
      "train loss:0.02117197038083463\n",
      "train loss:0.039735800216014916\n",
      "train loss:0.014645636874160253\n",
      "train loss:0.015968062062932324\n",
      "=== epoch:18, train acc:0.99, test acc:0.963 ===\n",
      "train loss:0.024358277813058816\n",
      "train loss:0.09651437503655215\n",
      "train loss:0.08710783740748354\n",
      "train loss:0.0134693808880444\n",
      "train loss:0.03303288932100768\n",
      "train loss:0.022757969979161064\n",
      "train loss:0.08248306672268496\n",
      "train loss:0.016888169166154595\n",
      "train loss:0.05534635051541215\n",
      "train loss:0.03103889169020256\n",
      "train loss:0.015028403114128428\n",
      "train loss:0.03183368635559809\n",
      "train loss:0.01682643733240929\n",
      "train loss:0.016643320442804036\n",
      "train loss:0.029569040578067383\n",
      "train loss:0.021850192479307747\n",
      "train loss:0.05258702229925362\n",
      "train loss:0.014045864443235386\n",
      "train loss:0.052739046685641056\n",
      "train loss:0.01547724126262037\n",
      "train loss:0.025036538574260067\n",
      "train loss:0.028292231353723284\n",
      "train loss:0.020771663353677944\n",
      "train loss:0.022634727841870417\n",
      "train loss:0.010296293859126835\n",
      "train loss:0.01211094380430242\n",
      "train loss:0.03879141792870816\n",
      "train loss:0.014825732324352532\n",
      "train loss:0.05488516029840614\n",
      "train loss:0.04139376382802379\n",
      "train loss:0.01254650795633537\n",
      "train loss:0.020917348756642494\n",
      "train loss:0.0302005821362384\n",
      "train loss:0.012545801015051865\n",
      "train loss:0.01889658000423418\n",
      "train loss:0.021456880256340693\n",
      "train loss:0.013871769226257118\n",
      "train loss:0.030606322593897207\n",
      "train loss:0.02534544463247379\n",
      "train loss:0.019947355820370917\n",
      "train loss:0.041826093203110766\n",
      "train loss:0.040703206368558946\n",
      "train loss:0.016204291213518254\n",
      "train loss:0.004699524812382946\n",
      "train loss:0.016707359297406386\n",
      "train loss:0.0544033883581099\n",
      "train loss:0.06976146360814824\n",
      "train loss:0.021582782551703136\n",
      "train loss:0.029159213462859646\n",
      "train loss:0.0430177344259092\n",
      "=== epoch:19, train acc:0.99, test acc:0.958 ===\n",
      "train loss:0.010127298178864527\n",
      "train loss:0.047474367951831645\n",
      "train loss:0.012787314814377077\n",
      "train loss:0.022764682011153493\n",
      "train loss:0.04049479477649899\n",
      "train loss:0.04855629806678954\n",
      "train loss:0.014462038350482941\n",
      "train loss:0.07899892081020195\n",
      "train loss:0.00774699794969168\n",
      "train loss:0.02260118791468956\n",
      "train loss:0.02694955744993353\n",
      "train loss:0.0758554078038108\n",
      "train loss:0.03775343770507116\n",
      "train loss:0.018897785540189414\n",
      "train loss:0.033397916628291946\n",
      "train loss:0.017651477928242704\n",
      "train loss:0.042178327508460185\n",
      "train loss:0.013280655376894943\n",
      "train loss:0.015054748407636528\n",
      "train loss:0.025098385889783466\n",
      "train loss:0.01433718455791984\n",
      "train loss:0.015139131369692405\n",
      "train loss:0.02605009372742026\n",
      "train loss:0.0366910515613186\n",
      "train loss:0.011864488052059322\n",
      "train loss:0.04490682161694126\n",
      "train loss:0.014015730507262599\n",
      "train loss:0.02965417390084394\n",
      "train loss:0.02548644675426419\n",
      "train loss:0.04551655638631677\n",
      "train loss:0.024345089967179007\n",
      "train loss:0.024346871590817675\n",
      "train loss:0.02772308982342469\n",
      "train loss:0.01737067765976661\n",
      "train loss:0.016913192496213626\n",
      "train loss:0.010159598455554092\n",
      "train loss:0.010984441507306208\n",
      "train loss:0.017257501310677967\n",
      "train loss:0.020324145192013466\n",
      "train loss:0.01866711919986309\n",
      "train loss:0.007939203178437656\n",
      "train loss:0.028655956309560457\n",
      "train loss:0.007962597744541633\n",
      "train loss:0.019193429008305472\n",
      "train loss:0.020160855450139698\n",
      "train loss:0.03294847616908528\n",
      "train loss:0.04813923288136206\n",
      "train loss:0.021948465503211712\n",
      "train loss:0.014593471605908974\n",
      "train loss:0.008173203709667405\n",
      "=== epoch:20, train acc:0.995, test acc:0.963 ===\n",
      "train loss:0.01741238121116912\n",
      "train loss:0.008450217090994378\n",
      "train loss:0.009619497896075328\n",
      "train loss:0.014081615582077973\n",
      "train loss:0.017265552807142058\n",
      "train loss:0.016114447348569424\n",
      "train loss:0.00648360646427691\n",
      "train loss:0.010549146291696258\n",
      "train loss:0.04414977762982319\n",
      "train loss:0.03829565114509871\n",
      "train loss:0.017341330158681334\n",
      "train loss:0.011973695860481597\n",
      "train loss:0.012684726284921108\n",
      "train loss:0.040909417361862384\n",
      "train loss:0.007202350865330848\n",
      "train loss:0.01821308651148346\n",
      "train loss:0.008908656554747462\n",
      "train loss:0.009689085506397802\n",
      "train loss:0.015042625972613544\n",
      "train loss:0.05854107982784752\n",
      "train loss:0.024006050563011517\n",
      "train loss:0.019301542934116053\n",
      "train loss:0.016556722087873704\n",
      "train loss:0.012410442390849999\n",
      "train loss:0.024826647331167673\n",
      "train loss:0.012897708137423186\n",
      "train loss:0.015693794896450272\n",
      "train loss:0.016356766517087243\n",
      "train loss:0.01307340049519979\n",
      "train loss:0.037822058273020806\n",
      "train loss:0.03004013628510419\n",
      "train loss:0.02380628450883531\n",
      "train loss:0.031819795609510954\n",
      "train loss:0.018059588598221436\n",
      "train loss:0.019074355837310833\n",
      "train loss:0.01735140364782909\n",
      "train loss:0.01310497472816601\n",
      "train loss:0.01589977663847286\n",
      "train loss:0.016398944147791677\n",
      "train loss:0.0246421299589412\n",
      "train loss:0.028111888121894267\n",
      "train loss:0.0296199494196427\n",
      "train loss:0.019973962608155264\n",
      "train loss:0.011071671576145184\n",
      "train loss:0.029511117681068795\n",
      "train loss:0.01908274079517629\n",
      "train loss:0.011438598838252417\n",
      "train loss:0.02711263597044848\n",
      "train loss:0.01773402836592501\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.95\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten = False)\n",
    "\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim = (1, 28, 28),\n",
    "                      conv_param = {'filter_num' : 30, 'filter_size' : 5,\n",
    "                                   'pad' : 0, 'stride' : 1},\n",
    "                      hidden_size = 100, output_size = 10, weight_init_std = 0.01)\n",
    "\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                 epochs = max_epochs, mini_batch_size = 100,\n",
    "                 optimizer = 'Adam', optimizer_param = {'lr' : 0.001},\n",
    "                 evaluate_sample_num_per_epoch = 1000)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e622aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977759db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3959a28d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1497c75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482a78f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd1934d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f62f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e39fb9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb97d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eac2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2794674a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88357b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7783d066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
